{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ga642381/ML2021-Spring/blob/main/HW01/HW01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mz0_QVkxCrX3"
   },
   "source": [
    "# **Homework 1: COVID-19 Cases Prediction (Regression)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZeZnPAiwDRWG"
   },
   "source": [
    "Author: Heng-Jui Chang\n",
    "\n",
    "Slides: https://github.com/ga642381/ML2021-Spring/blob/main/HW01/HW01.pdf  \n",
    "Videos (Mandarin): https://cool.ntu.edu.tw/courses/4793/modules/items/172854  \n",
    "https://cool.ntu.edu.tw/courses/4793/modules/items/172853  \n",
    "Video (English): https://cool.ntu.edu.tw/courses/4793/modules/items/176529\n",
    "\n",
    "\n",
    "Objectives:\n",
    "* Solve a regression problem with deep neural networks (DNN).\n",
    "* Understand basic DNN training tips.\n",
    "* Get familiar with PyTorch.\n",
    "\n",
    "If any questions, please contact the TAs via TA hours, NTU COOL, or email.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jx3x1nDkG-Uy"
   },
   "source": [
    "# **Download Data**\n",
    "\n",
    "\n",
    "If the Google drive links are dead, you can download data from [kaggle](https://www.kaggle.com/c/ml2021spring-hw1/data), and upload data manually to the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tMj55YDKG6ch",
    "outputId": "fc40ecc9-4756-48b1-d5c6-c169a8b453b2"
   },
   "outputs": [],
   "source": [
    "tr_path = 'covid.train.csv'  # path to training data\n",
    "tt_path = 'covid.test.csv'   # path to testing data\n",
    "\n",
    "# !gdown --id '19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF' --output covid.train.csv\n",
    "# !gdown --id '1CE240jLm2npU-tdz81-oVKEF3T2yfT1O' --output covid.test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wS_4-77xHk44"
   },
   "source": [
    "# **Import Some Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "k-onQd4JNA5H"
   },
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# For data preprocess\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "myseed = 42069  # set a random seed for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(myseed)\n",
    "torch.manual_seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv(tr_path)\n",
    "test_data = pd.read_csv(tt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cli</th>\n",
       "      <th>ili</th>\n",
       "      <th>hh_cmnty_cli</th>\n",
       "      <th>nohh_cmnty_cli</th>\n",
       "      <th>wearing_mask</th>\n",
       "      <th>travel_outside_state</th>\n",
       "      <th>work_outside_home</th>\n",
       "      <th>shop</th>\n",
       "      <th>restaurant</th>\n",
       "      <th>spent_time</th>\n",
       "      <th>...</th>\n",
       "      <th>restaurant.2</th>\n",
       "      <th>spent_time.2</th>\n",
       "      <th>large_event.2</th>\n",
       "      <th>public_transit.2</th>\n",
       "      <th>anxious.2</th>\n",
       "      <th>depressed.2</th>\n",
       "      <th>felt_isolated.2</th>\n",
       "      <th>worried_become_ill.2</th>\n",
       "      <th>worried_finances.2</th>\n",
       "      <th>tested_positive.2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.814610</td>\n",
       "      <td>0.771356</td>\n",
       "      <td>25.648907</td>\n",
       "      <td>21.242063</td>\n",
       "      <td>84.644672</td>\n",
       "      <td>13.462475</td>\n",
       "      <td>36.519841</td>\n",
       "      <td>63.139094</td>\n",
       "      <td>23.835119</td>\n",
       "      <td>44.726055</td>\n",
       "      <td>...</td>\n",
       "      <td>23.812411</td>\n",
       "      <td>43.430423</td>\n",
       "      <td>16.151527</td>\n",
       "      <td>1.602635</td>\n",
       "      <td>15.409449</td>\n",
       "      <td>12.088688</td>\n",
       "      <td>16.702086</td>\n",
       "      <td>53.991549</td>\n",
       "      <td>43.604229</td>\n",
       "      <td>20.704935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.838995</td>\n",
       "      <td>0.807766</td>\n",
       "      <td>25.679101</td>\n",
       "      <td>21.280270</td>\n",
       "      <td>84.005294</td>\n",
       "      <td>13.467716</td>\n",
       "      <td>36.637887</td>\n",
       "      <td>63.318650</td>\n",
       "      <td>23.688882</td>\n",
       "      <td>44.385166</td>\n",
       "      <td>...</td>\n",
       "      <td>23.682974</td>\n",
       "      <td>43.196313</td>\n",
       "      <td>16.123386</td>\n",
       "      <td>1.641863</td>\n",
       "      <td>15.230063</td>\n",
       "      <td>11.809047</td>\n",
       "      <td>16.506973</td>\n",
       "      <td>54.185521</td>\n",
       "      <td>42.665766</td>\n",
       "      <td>21.292911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.897801</td>\n",
       "      <td>0.887893</td>\n",
       "      <td>26.060544</td>\n",
       "      <td>21.503832</td>\n",
       "      <td>84.438618</td>\n",
       "      <td>13.038611</td>\n",
       "      <td>36.429119</td>\n",
       "      <td>62.434539</td>\n",
       "      <td>23.812411</td>\n",
       "      <td>43.430423</td>\n",
       "      <td>...</td>\n",
       "      <td>23.593983</td>\n",
       "      <td>43.362200</td>\n",
       "      <td>16.159971</td>\n",
       "      <td>1.677523</td>\n",
       "      <td>15.717207</td>\n",
       "      <td>12.355918</td>\n",
       "      <td>16.273294</td>\n",
       "      <td>53.637069</td>\n",
       "      <td>42.972417</td>\n",
       "      <td>21.166656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.972842</td>\n",
       "      <td>0.965496</td>\n",
       "      <td>25.754087</td>\n",
       "      <td>21.016210</td>\n",
       "      <td>84.133873</td>\n",
       "      <td>12.581952</td>\n",
       "      <td>36.416557</td>\n",
       "      <td>62.024517</td>\n",
       "      <td>23.682974</td>\n",
       "      <td>43.196313</td>\n",
       "      <td>...</td>\n",
       "      <td>22.576992</td>\n",
       "      <td>42.954574</td>\n",
       "      <td>15.544373</td>\n",
       "      <td>1.578030</td>\n",
       "      <td>15.295650</td>\n",
       "      <td>12.218123</td>\n",
       "      <td>16.045504</td>\n",
       "      <td>52.446223</td>\n",
       "      <td>42.907472</td>\n",
       "      <td>19.896607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.955306</td>\n",
       "      <td>0.963079</td>\n",
       "      <td>25.947015</td>\n",
       "      <td>20.941798</td>\n",
       "      <td>83.995931</td>\n",
       "      <td>12.938675</td>\n",
       "      <td>37.014578</td>\n",
       "      <td>62.116843</td>\n",
       "      <td>23.593983</td>\n",
       "      <td>43.362200</td>\n",
       "      <td>...</td>\n",
       "      <td>22.091433</td>\n",
       "      <td>43.290957</td>\n",
       "      <td>15.214655</td>\n",
       "      <td>1.641667</td>\n",
       "      <td>14.778802</td>\n",
       "      <td>12.417256</td>\n",
       "      <td>16.134238</td>\n",
       "      <td>52.560315</td>\n",
       "      <td>43.321985</td>\n",
       "      <td>20.178428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>0.655823</td>\n",
       "      <td>0.659976</td>\n",
       "      <td>25.265366</td>\n",
       "      <td>20.468897</td>\n",
       "      <td>91.011756</td>\n",
       "      <td>6.801897</td>\n",
       "      <td>32.727184</td>\n",
       "      <td>50.265694</td>\n",
       "      <td>15.188547</td>\n",
       "      <td>31.597793</td>\n",
       "      <td>...</td>\n",
       "      <td>15.090116</td>\n",
       "      <td>30.839219</td>\n",
       "      <td>7.849525</td>\n",
       "      <td>1.760094</td>\n",
       "      <td>14.617563</td>\n",
       "      <td>11.163213</td>\n",
       "      <td>18.742673</td>\n",
       "      <td>68.024690</td>\n",
       "      <td>38.920206</td>\n",
       "      <td>13.008853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>0.598352</td>\n",
       "      <td>0.602552</td>\n",
       "      <td>25.299465</td>\n",
       "      <td>20.756444</td>\n",
       "      <td>90.682057</td>\n",
       "      <td>7.152368</td>\n",
       "      <td>33.638563</td>\n",
       "      <td>50.050349</td>\n",
       "      <td>15.462823</td>\n",
       "      <td>31.656358</td>\n",
       "      <td>...</td>\n",
       "      <td>14.779264</td>\n",
       "      <td>30.617100</td>\n",
       "      <td>7.754800</td>\n",
       "      <td>1.780730</td>\n",
       "      <td>14.513419</td>\n",
       "      <td>11.281241</td>\n",
       "      <td>18.539741</td>\n",
       "      <td>67.855755</td>\n",
       "      <td>39.224244</td>\n",
       "      <td>12.725638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>0.586713</td>\n",
       "      <td>0.597559</td>\n",
       "      <td>25.271178</td>\n",
       "      <td>20.770195</td>\n",
       "      <td>90.866100</td>\n",
       "      <td>6.857209</td>\n",
       "      <td>33.959012</td>\n",
       "      <td>50.024971</td>\n",
       "      <td>15.090116</td>\n",
       "      <td>30.839219</td>\n",
       "      <td>...</td>\n",
       "      <td>14.961085</td>\n",
       "      <td>30.595194</td>\n",
       "      <td>7.744075</td>\n",
       "      <td>1.921828</td>\n",
       "      <td>14.160990</td>\n",
       "      <td>11.163526</td>\n",
       "      <td>18.702564</td>\n",
       "      <td>67.731162</td>\n",
       "      <td>38.740651</td>\n",
       "      <td>12.613441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>0.576435</td>\n",
       "      <td>0.595312</td>\n",
       "      <td>24.607461</td>\n",
       "      <td>20.176201</td>\n",
       "      <td>90.846126</td>\n",
       "      <td>6.851475</td>\n",
       "      <td>33.932384</td>\n",
       "      <td>49.885129</td>\n",
       "      <td>14.779264</td>\n",
       "      <td>30.617100</td>\n",
       "      <td>...</td>\n",
       "      <td>14.609582</td>\n",
       "      <td>30.420998</td>\n",
       "      <td>7.687974</td>\n",
       "      <td>1.992580</td>\n",
       "      <td>14.409427</td>\n",
       "      <td>11.330301</td>\n",
       "      <td>19.134697</td>\n",
       "      <td>67.795100</td>\n",
       "      <td>38.595125</td>\n",
       "      <td>12.477227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>0.562426</td>\n",
       "      <td>0.572969</td>\n",
       "      <td>24.020275</td>\n",
       "      <td>19.654514</td>\n",
       "      <td>90.928655</td>\n",
       "      <td>6.642911</td>\n",
       "      <td>33.822577</td>\n",
       "      <td>50.056772</td>\n",
       "      <td>14.961085</td>\n",
       "      <td>30.595194</td>\n",
       "      <td>...</td>\n",
       "      <td>14.464053</td>\n",
       "      <td>30.469791</td>\n",
       "      <td>7.692942</td>\n",
       "      <td>1.966064</td>\n",
       "      <td>14.616400</td>\n",
       "      <td>11.522773</td>\n",
       "      <td>19.295834</td>\n",
       "      <td>68.284078</td>\n",
       "      <td>38.453820</td>\n",
       "      <td>11.811719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2700 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           cli       ili  hh_cmnty_cli  nohh_cmnty_cli  wearing_mask  \\\n",
       "0     0.814610  0.771356     25.648907       21.242063     84.644672   \n",
       "1     0.838995  0.807766     25.679101       21.280270     84.005294   \n",
       "2     0.897801  0.887893     26.060544       21.503832     84.438618   \n",
       "3     0.972842  0.965496     25.754087       21.016210     84.133873   \n",
       "4     0.955306  0.963079     25.947015       20.941798     83.995931   \n",
       "...        ...       ...           ...             ...           ...   \n",
       "2695  0.655823  0.659976     25.265366       20.468897     91.011756   \n",
       "2696  0.598352  0.602552     25.299465       20.756444     90.682057   \n",
       "2697  0.586713  0.597559     25.271178       20.770195     90.866100   \n",
       "2698  0.576435  0.595312     24.607461       20.176201     90.846126   \n",
       "2699  0.562426  0.572969     24.020275       19.654514     90.928655   \n",
       "\n",
       "      travel_outside_state  work_outside_home       shop  restaurant  \\\n",
       "0                13.462475          36.519841  63.139094   23.835119   \n",
       "1                13.467716          36.637887  63.318650   23.688882   \n",
       "2                13.038611          36.429119  62.434539   23.812411   \n",
       "3                12.581952          36.416557  62.024517   23.682974   \n",
       "4                12.938675          37.014578  62.116843   23.593983   \n",
       "...                    ...                ...        ...         ...   \n",
       "2695              6.801897          32.727184  50.265694   15.188547   \n",
       "2696              7.152368          33.638563  50.050349   15.462823   \n",
       "2697              6.857209          33.959012  50.024971   15.090116   \n",
       "2698              6.851475          33.932384  49.885129   14.779264   \n",
       "2699              6.642911          33.822577  50.056772   14.961085   \n",
       "\n",
       "      spent_time  ...  restaurant.2  spent_time.2  large_event.2  \\\n",
       "0      44.726055  ...     23.812411     43.430423      16.151527   \n",
       "1      44.385166  ...     23.682974     43.196313      16.123386   \n",
       "2      43.430423  ...     23.593983     43.362200      16.159971   \n",
       "3      43.196313  ...     22.576992     42.954574      15.544373   \n",
       "4      43.362200  ...     22.091433     43.290957      15.214655   \n",
       "...          ...  ...           ...           ...            ...   \n",
       "2695   31.597793  ...     15.090116     30.839219       7.849525   \n",
       "2696   31.656358  ...     14.779264     30.617100       7.754800   \n",
       "2697   30.839219  ...     14.961085     30.595194       7.744075   \n",
       "2698   30.617100  ...     14.609582     30.420998       7.687974   \n",
       "2699   30.595194  ...     14.464053     30.469791       7.692942   \n",
       "\n",
       "      public_transit.2  anxious.2  depressed.2  felt_isolated.2  \\\n",
       "0             1.602635  15.409449    12.088688        16.702086   \n",
       "1             1.641863  15.230063    11.809047        16.506973   \n",
       "2             1.677523  15.717207    12.355918        16.273294   \n",
       "3             1.578030  15.295650    12.218123        16.045504   \n",
       "4             1.641667  14.778802    12.417256        16.134238   \n",
       "...                ...        ...          ...              ...   \n",
       "2695          1.760094  14.617563    11.163213        18.742673   \n",
       "2696          1.780730  14.513419    11.281241        18.539741   \n",
       "2697          1.921828  14.160990    11.163526        18.702564   \n",
       "2698          1.992580  14.409427    11.330301        19.134697   \n",
       "2699          1.966064  14.616400    11.522773        19.295834   \n",
       "\n",
       "      worried_become_ill.2  worried_finances.2  tested_positive.2  \n",
       "0                53.991549           43.604229          20.704935  \n",
       "1                54.185521           42.665766          21.292911  \n",
       "2                53.637069           42.972417          21.166656  \n",
       "3                52.446223           42.907472          19.896607  \n",
       "4                52.560315           43.321985          20.178428  \n",
       "...                    ...                 ...                ...  \n",
       "2695             68.024690           38.920206          13.008853  \n",
       "2696             67.855755           39.224244          12.725638  \n",
       "2697             67.731162           38.740651          12.613441  \n",
       "2698             67.795100           38.595125          12.477227  \n",
       "2699             68.284078           38.453820          11.811719  \n",
       "\n",
       "[2700 rows x 54 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.iloc[:, 41:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cli</th>\n",
       "      <th>ili</th>\n",
       "      <th>hh_cmnty_cli</th>\n",
       "      <th>nohh_cmnty_cli</th>\n",
       "      <th>wearing_mask</th>\n",
       "      <th>travel_outside_state</th>\n",
       "      <th>work_outside_home</th>\n",
       "      <th>shop</th>\n",
       "      <th>restaurant</th>\n",
       "      <th>spent_time</th>\n",
       "      <th>...</th>\n",
       "      <th>restaurant.2</th>\n",
       "      <th>spent_time.2</th>\n",
       "      <th>large_event.2</th>\n",
       "      <th>public_transit.2</th>\n",
       "      <th>anxious.2</th>\n",
       "      <th>depressed.2</th>\n",
       "      <th>felt_isolated.2</th>\n",
       "      <th>worried_become_ill.2</th>\n",
       "      <th>worried_finances.2</th>\n",
       "      <th>tested_positive.2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2700.000000</td>\n",
       "      <td>2700.000000</td>\n",
       "      <td>2700.000000</td>\n",
       "      <td>2700.000000</td>\n",
       "      <td>2700.000000</td>\n",
       "      <td>2700.000000</td>\n",
       "      <td>2700.000000</td>\n",
       "      <td>2700.000000</td>\n",
       "      <td>2700.000000</td>\n",
       "      <td>2700.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2700.000000</td>\n",
       "      <td>2700.000000</td>\n",
       "      <td>2700.000000</td>\n",
       "      <td>2700.000000</td>\n",
       "      <td>2700.000000</td>\n",
       "      <td>2700.000000</td>\n",
       "      <td>2700.000000</td>\n",
       "      <td>2700.000000</td>\n",
       "      <td>2700.000000</td>\n",
       "      <td>2700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.991587</td>\n",
       "      <td>1.016136</td>\n",
       "      <td>29.442496</td>\n",
       "      <td>24.323054</td>\n",
       "      <td>89.682322</td>\n",
       "      <td>8.894498</td>\n",
       "      <td>31.703307</td>\n",
       "      <td>55.277153</td>\n",
       "      <td>16.694342</td>\n",
       "      <td>36.283177</td>\n",
       "      <td>...</td>\n",
       "      <td>16.578290</td>\n",
       "      <td>36.074941</td>\n",
       "      <td>10.257474</td>\n",
       "      <td>2.385735</td>\n",
       "      <td>18.067635</td>\n",
       "      <td>13.058828</td>\n",
       "      <td>19.243283</td>\n",
       "      <td>64.834307</td>\n",
       "      <td>44.568440</td>\n",
       "      <td>16.431280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.420296</td>\n",
       "      <td>0.423629</td>\n",
       "      <td>9.093738</td>\n",
       "      <td>8.446750</td>\n",
       "      <td>5.380027</td>\n",
       "      <td>3.404027</td>\n",
       "      <td>4.928902</td>\n",
       "      <td>4.525917</td>\n",
       "      <td>5.668479</td>\n",
       "      <td>6.675206</td>\n",
       "      <td>...</td>\n",
       "      <td>5.651583</td>\n",
       "      <td>6.655166</td>\n",
       "      <td>4.686263</td>\n",
       "      <td>1.053147</td>\n",
       "      <td>2.250081</td>\n",
       "      <td>1.628589</td>\n",
       "      <td>2.708339</td>\n",
       "      <td>6.220087</td>\n",
       "      <td>5.232030</td>\n",
       "      <td>7.619354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.126321</td>\n",
       "      <td>0.132470</td>\n",
       "      <td>9.961640</td>\n",
       "      <td>6.857181</td>\n",
       "      <td>70.950912</td>\n",
       "      <td>1.252983</td>\n",
       "      <td>18.311941</td>\n",
       "      <td>43.220187</td>\n",
       "      <td>3.637414</td>\n",
       "      <td>21.485815</td>\n",
       "      <td>...</td>\n",
       "      <td>3.637414</td>\n",
       "      <td>21.485815</td>\n",
       "      <td>2.118674</td>\n",
       "      <td>0.728770</td>\n",
       "      <td>12.980786</td>\n",
       "      <td>8.370536</td>\n",
       "      <td>13.400399</td>\n",
       "      <td>48.225603</td>\n",
       "      <td>33.113882</td>\n",
       "      <td>2.338708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.673929</td>\n",
       "      <td>0.697515</td>\n",
       "      <td>23.203165</td>\n",
       "      <td>18.539153</td>\n",
       "      <td>86.309537</td>\n",
       "      <td>6.177754</td>\n",
       "      <td>28.247865</td>\n",
       "      <td>51.547206</td>\n",
       "      <td>13.311050</td>\n",
       "      <td>30.740931</td>\n",
       "      <td>...</td>\n",
       "      <td>13.200532</td>\n",
       "      <td>30.606711</td>\n",
       "      <td>6.532543</td>\n",
       "      <td>1.714080</td>\n",
       "      <td>16.420485</td>\n",
       "      <td>11.914167</td>\n",
       "      <td>17.322912</td>\n",
       "      <td>59.782876</td>\n",
       "      <td>40.549987</td>\n",
       "      <td>10.327314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.912747</td>\n",
       "      <td>0.940295</td>\n",
       "      <td>28.955738</td>\n",
       "      <td>23.819761</td>\n",
       "      <td>90.819435</td>\n",
       "      <td>8.288288</td>\n",
       "      <td>32.143140</td>\n",
       "      <td>55.257262</td>\n",
       "      <td>16.371699</td>\n",
       "      <td>36.267966</td>\n",
       "      <td>...</td>\n",
       "      <td>16.227010</td>\n",
       "      <td>36.041389</td>\n",
       "      <td>9.700368</td>\n",
       "      <td>2.199521</td>\n",
       "      <td>17.684197</td>\n",
       "      <td>12.948749</td>\n",
       "      <td>18.760267</td>\n",
       "      <td>65.932258</td>\n",
       "      <td>43.997637</td>\n",
       "      <td>15.646480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.266849</td>\n",
       "      <td>1.302040</td>\n",
       "      <td>36.109114</td>\n",
       "      <td>30.238061</td>\n",
       "      <td>93.937119</td>\n",
       "      <td>11.582209</td>\n",
       "      <td>35.387315</td>\n",
       "      <td>58.866130</td>\n",
       "      <td>21.396971</td>\n",
       "      <td>41.659971</td>\n",
       "      <td>...</td>\n",
       "      <td>21.207162</td>\n",
       "      <td>41.508520</td>\n",
       "      <td>13.602566</td>\n",
       "      <td>2.730469</td>\n",
       "      <td>19.503419</td>\n",
       "      <td>14.214320</td>\n",
       "      <td>20.713638</td>\n",
       "      <td>69.719651</td>\n",
       "      <td>48.118283</td>\n",
       "      <td>22.535165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.597732</td>\n",
       "      <td>2.625885</td>\n",
       "      <td>56.832289</td>\n",
       "      <td>51.550450</td>\n",
       "      <td>98.087160</td>\n",
       "      <td>18.552325</td>\n",
       "      <td>42.359074</td>\n",
       "      <td>65.673889</td>\n",
       "      <td>28.488220</td>\n",
       "      <td>50.606465</td>\n",
       "      <td>...</td>\n",
       "      <td>28.488220</td>\n",
       "      <td>50.606465</td>\n",
       "      <td>24.496711</td>\n",
       "      <td>8.162275</td>\n",
       "      <td>28.574091</td>\n",
       "      <td>18.715944</td>\n",
       "      <td>28.366270</td>\n",
       "      <td>77.701014</td>\n",
       "      <td>58.433600</td>\n",
       "      <td>40.959495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               cli          ili  hh_cmnty_cli  nohh_cmnty_cli  wearing_mask  \\\n",
       "count  2700.000000  2700.000000   2700.000000     2700.000000   2700.000000   \n",
       "mean      0.991587     1.016136     29.442496       24.323054     89.682322   \n",
       "std       0.420296     0.423629      9.093738        8.446750      5.380027   \n",
       "min       0.126321     0.132470      9.961640        6.857181     70.950912   \n",
       "25%       0.673929     0.697515     23.203165       18.539153     86.309537   \n",
       "50%       0.912747     0.940295     28.955738       23.819761     90.819435   \n",
       "75%       1.266849     1.302040     36.109114       30.238061     93.937119   \n",
       "max       2.597732     2.625885     56.832289       51.550450     98.087160   \n",
       "\n",
       "       travel_outside_state  work_outside_home         shop   restaurant  \\\n",
       "count           2700.000000        2700.000000  2700.000000  2700.000000   \n",
       "mean               8.894498          31.703307    55.277153    16.694342   \n",
       "std                3.404027           4.928902     4.525917     5.668479   \n",
       "min                1.252983          18.311941    43.220187     3.637414   \n",
       "25%                6.177754          28.247865    51.547206    13.311050   \n",
       "50%                8.288288          32.143140    55.257262    16.371699   \n",
       "75%               11.582209          35.387315    58.866130    21.396971   \n",
       "max               18.552325          42.359074    65.673889    28.488220   \n",
       "\n",
       "        spent_time  ...  restaurant.2  spent_time.2  large_event.2  \\\n",
       "count  2700.000000  ...   2700.000000   2700.000000    2700.000000   \n",
       "mean     36.283177  ...     16.578290     36.074941      10.257474   \n",
       "std       6.675206  ...      5.651583      6.655166       4.686263   \n",
       "min      21.485815  ...      3.637414     21.485815       2.118674   \n",
       "25%      30.740931  ...     13.200532     30.606711       6.532543   \n",
       "50%      36.267966  ...     16.227010     36.041389       9.700368   \n",
       "75%      41.659971  ...     21.207162     41.508520      13.602566   \n",
       "max      50.606465  ...     28.488220     50.606465      24.496711   \n",
       "\n",
       "       public_transit.2    anxious.2  depressed.2  felt_isolated.2  \\\n",
       "count       2700.000000  2700.000000  2700.000000      2700.000000   \n",
       "mean           2.385735    18.067635    13.058828        19.243283   \n",
       "std            1.053147     2.250081     1.628589         2.708339   \n",
       "min            0.728770    12.980786     8.370536        13.400399   \n",
       "25%            1.714080    16.420485    11.914167        17.322912   \n",
       "50%            2.199521    17.684197    12.948749        18.760267   \n",
       "75%            2.730469    19.503419    14.214320        20.713638   \n",
       "max            8.162275    28.574091    18.715944        28.366270   \n",
       "\n",
       "       worried_become_ill.2  worried_finances.2  tested_positive.2  \n",
       "count           2700.000000         2700.000000        2700.000000  \n",
       "mean              64.834307           44.568440          16.431280  \n",
       "std                6.220087            5.232030           7.619354  \n",
       "min               48.225603           33.113882           2.338708  \n",
       "25%               59.782876           40.549987          10.327314  \n",
       "50%               65.932258           43.997637          15.646480  \n",
       "75%               69.719651           48.118283          22.535165  \n",
       "max               77.701014           58.433600          40.959495  \n",
       "\n",
       "[8 rows x 54 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.iloc[:, 41:].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cli                   0.838504\n",
      "ili                   0.830527\n",
      "hh_cmnty_cli          0.879724\n",
      "nohh_cmnty_cli        0.869938\n",
      "shop                 -0.410430\n",
      "public_transit       -0.448360\n",
      "worried_finances      0.475462\n",
      "tested_positive       0.981165\n",
      "cli.1                 0.838224\n",
      "ili.1                 0.829200\n",
      "hh_cmnty_cli.1        0.879438\n",
      "nohh_cmnty_cli.1      0.869278\n",
      "shop.1               -0.412705\n",
      "public_transit.1     -0.449079\n",
      "worried_finances.1    0.480958\n",
      "tested_positive.1     0.991012\n",
      "cli.2                 0.835751\n",
      "ili.2                 0.826075\n",
      "hh_cmnty_cli.2        0.878218\n",
      "nohh_cmnty_cli.2      0.867535\n",
      "shop.2               -0.415130\n",
      "public_transit.2     -0.450436\n",
      "worried_finances.2    0.485843\n",
      "tested_positive.2     1.000000\n",
      "Name: tested_positive.2, dtype: float64\n",
      "\n",
      "features' id: [40 41 42 43 47 51 56 57 58 59 60 61 65 69 74 75 76 77 78 79 83 87 92]\n"
     ]
    }
   ],
   "source": [
    "corr = train_data.iloc[:, 41:].corr().iloc[-1]\n",
    "features = corr[abs(corr) > 0.4]\n",
    "features_col = features.index.to_list()[:-1]\n",
    "features_id = np.array([train_data.columns.to_list().index(i) for i in features_col]) - 1\n",
    "\n",
    "print (features)\n",
    "print (\"\\nfeatures' id:\", features_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [i for i in range(40)] + list(features_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_level = train_data.iloc[:, 41:-1].min().values\n",
    "max_level = train_data.iloc[:, 41:-1].max().values\n",
    "mean_level = train_data.iloc[:, 41:-1].mean().values\n",
    "std_level = train_data.iloc[:, 41:-1].std().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtE3b6JEH7rw"
   },
   "source": [
    "# **Some Utilities**\n",
    "\n",
    "You do not need to modify this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FWMT3uf1NGQp"
   },
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    ''' Get device (if GPU is available, use GPU) '''\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def plot_learning_curve(loss_record, title=''):\n",
    "    ''' Plot learning curve of your DNN (train & dev loss) '''\n",
    "    total_steps = len(loss_record['train'])\n",
    "    x_1 = range(total_steps)\n",
    "    x_2 = x_1[::len(loss_record['train']) // len(loss_record['dev'])]\n",
    "    figure(figsize=(6, 4))\n",
    "    plt.plot(x_1, loss_record['train'], c='tab:red', label='train')\n",
    "    plt.plot(x_2, loss_record['dev'], c='tab:cyan', label='dev')\n",
    "    plt.ylim(0.0, 5.)\n",
    "    plt.xlabel('Training steps')\n",
    "    plt.ylabel('MSE loss')\n",
    "    plt.title('Learning curve of {}'.format(title))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_pred(dv_set, model, device, lim=35., preds=None, targets=None):\n",
    "    ''' Plot prediction of your DNN '''\n",
    "    if preds is None or targets is None:\n",
    "        model.eval()\n",
    "        preds, targets = [], []\n",
    "        for x, y in dv_set:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(x)\n",
    "                preds.append(pred.detach().cpu())\n",
    "                targets.append(y.detach().cpu())\n",
    "        preds = torch.cat(preds, dim=0).numpy()\n",
    "        targets = torch.cat(targets, dim=0).numpy()\n",
    "\n",
    "    figure(figsize=(5, 5))\n",
    "    plt.scatter(targets, preds, c='r', alpha=0.5)\n",
    "    plt.plot([-0.2, lim], [-0.2, lim], c='b')\n",
    "    plt.xlim(-0.2, lim)\n",
    "    plt.ylim(-0.2, lim)\n",
    "    plt.xlabel('ground truth value')\n",
    "    plt.ylabel('predicted value')\n",
    "    plt.title('Ground Truth v.s. Prediction')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39U_XFX6KOoj"
   },
   "source": [
    "# **Preprocess**\n",
    "\n",
    "We have three kinds of datasets:\n",
    "* `train`: for training\n",
    "* `dev`: for validation\n",
    "* `test`: for testing (w/o target value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQ-MdwpLL7Dt"
   },
   "source": [
    "## **Dataset**\n",
    "\n",
    "The `COVID19Dataset` below does:\n",
    "* read `.csv` files\n",
    "* extract features\n",
    "* split `covid.train.csv` into train/dev sets\n",
    "* normalize features\n",
    "\n",
    "Finishing `TODO` below might make you pass medium baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0zlpIp9ANJRU"
   },
   "outputs": [],
   "source": [
    "class COVID19Dataset(Dataset):\n",
    "    ''' Dataset for loading and preprocessing the COVID19 dataset '''\n",
    "    def __init__(self,\n",
    "                 path,\n",
    "                 mode='train',\n",
    "                 target_only=False,\n",
    "                 features_id = feats,\n",
    "                 min_level = min_level,\n",
    "                 max_level = max_level,\n",
    "                 mean_level = mean_level,\n",
    "                 std_level = std_level):\n",
    "        self.mode = mode\n",
    "\n",
    "        # Read data into numpy arrays\n",
    "        with open(path, 'r') as fp:\n",
    "            data = list(csv.reader(fp))\n",
    "            data = np.array(data[1:])[:, 1:].astype(float)\n",
    "            data[:, 40:93] = (data[:, 40:93] - min_level)/(max_level - min_level) # min-max normalization\n",
    "#             data[:, 40:93] = (data[:, 40:93] - mean_level)/std_level              #gaussian normalization\n",
    "        \n",
    "        if not target_only:\n",
    "            feats = list(range(93))\n",
    "        else:\n",
    "            # TODO: Using 40 states & 2 tested_positive features (indices = 57 & 75)\n",
    "            feats = features_id\n",
    "\n",
    "        if mode == 'test':\n",
    "            # Testing data\n",
    "            # data: 893 x 93 (40 states + day 1 (18) + day 2 (18) + day 3 (17))\n",
    "            data = data[:, feats]\n",
    "            self.data = torch.FloatTensor(data)\n",
    "        else:\n",
    "            # Training data (train/dev sets)\n",
    "            # data: 2700 x 94 (40 states + day 1 (18) + day 2 (18) + day 3 (18))\n",
    "            target = data[:, -1]\n",
    "            data = data[:, feats]\n",
    "            \n",
    "            # Splitting training data into train & dev sets\n",
    "            if mode == 'train':\n",
    "                indices = [i for i in range(len(data)) if i % 10 != 0]\n",
    "            elif mode == 'dev':\n",
    "                indices = [i for i in range(len(data)) if i % 10 == 0]\n",
    "            \n",
    "            # Convert data into PyTorch tensors\n",
    "            self.data = torch.FloatTensor(data[indices])\n",
    "            self.target = torch.FloatTensor(target[indices])\n",
    "\n",
    "        # Normalize features (you may remove this part to see what will happen)\n",
    "#         self.data[:, 40:] = \\\n",
    "#             (self.data[:, 40:] - self.data[:, 40:].mean(dim=0, keepdim=True)) \\\n",
    "#             / self.data[:, 40:].std(dim=0, keepdim=True)\n",
    "\n",
    "        self.dim = self.data.shape[1]\n",
    "\n",
    "        print('Finished reading the {} set of COVID19 Dataset ({} samples found, each dim = {})'\n",
    "              .format(mode, len(self.data), self.dim))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Returns one sample at a time\n",
    "        if self.mode in ['train', 'dev']:\n",
    "            # For training\n",
    "            return self.data[index], self.target[index]\n",
    "        else:\n",
    "            # For testing (no target)\n",
    "            return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns the size of the dataset\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlhTlkE7MDo3"
   },
   "source": [
    "## **DataLoader**\n",
    "\n",
    "A `DataLoader` loads data from a given `Dataset` into batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hlhLk5t6MBX3"
   },
   "outputs": [],
   "source": [
    "def prep_dataloader(path, mode, batch_size, n_jobs=0, target_only=False):\n",
    "    ''' Generates a dataset, then is put into a dataloader. '''\n",
    "    dataset = COVID19Dataset(path, mode=mode, target_only=target_only)  # Construct dataset\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size,\n",
    "        shuffle=(mode == 'train'), drop_last=False,\n",
    "        num_workers=n_jobs, pin_memory=True)                            # Construct dataloader\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGuycwR0MeQB"
   },
   "source": [
    "# **Deep Neural Network**\n",
    "\n",
    "`NeuralNet` is an `nn.Module` designed for regression.\n",
    "The DNN consists of 2 fully-connected layers with ReLU activation.\n",
    "This module also included a function `cal_loss` for calculating loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "49-uXYovOAI0"
   },
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    ''' A simple fully-connected deep neural network '''\n",
    "    def __init__(self, input_dim):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        # Define your neural network here\n",
    "        # TODO: How to modify this model to achieve better performance?\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 1)\n",
    "        )\n",
    "\n",
    "        # Mean squared error loss\n",
    "        self.criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' Given input of size (batch_size x input_dim), compute output of the network '''\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "    def cal_loss(self, pred, target):\n",
    "        \"\"\" Calculate loss \"\"\"\n",
    "        loss = torch.sqrt(self.criterion(pred, target))\n",
    "        # l2 = 0\n",
    "        # for i in self.parameters():\n",
    "        #     l2 += torch.sum(torch.pow(i, 2))\n",
    "        # return loss + 0.01 * l2, loss  \n",
    "        l1 = 0\n",
    "        for i in self.parameters():\n",
    "            l1 += torch.sum(abs(i))\n",
    "        return loss + 0.0005 * l1, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvFWVjZ5Nvga"
   },
   "source": [
    "# **Train/Dev/Test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAM8QecJOyqn"
   },
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lOqcmYzMO7jB"
   },
   "outputs": [],
   "source": [
    "def train(tr_set, dv_set, model, config, device):\n",
    "    ''' DNN training '''\n",
    "\n",
    "    n_epochs = config['n_epochs']  # Maximum number of epochs\n",
    "\n",
    "    # Setup optimizer\n",
    "    optimizer = getattr(torch.optim, config['optimizer'])(\n",
    "        model.parameters(), **config['optim_hparas'])\n",
    "\n",
    "    min_rmse = 1000.\n",
    "    loss_record = {'train': [], 'dev': []}      # for recording training loss\n",
    "    early_stop_cnt = 0\n",
    "    epoch = 0\n",
    "    while epoch < n_epochs:\n",
    "        model.train()                           # set model to training mode\n",
    "        for x, y in tr_set:                     # iterate through the dataloader\n",
    "            optimizer.zero_grad()               # set gradient to zero\n",
    "            x, y = x.to(device), y.to(device)   # move data to device (cpu/cuda)\n",
    "            pred = model(x)                     # forward pass (compute output)\n",
    "            rmse_l1_loss, rmse_loss = model.cal_loss(pred, y)  # compute loss\n",
    "            rmse_l1_loss.backward()                 # compute gradient (backpropagation)\n",
    "            optimizer.step()                    # update model with optimizer\n",
    "            train_loss = rmse_loss.detach().cpu().item() #train loss value\n",
    "            loss_record['train'].append(train_loss)\n",
    "\n",
    "        # After each epoch, test your model on the validation (development) set.\n",
    "        dev_rmse = dev(dv_set, model, device)\n",
    "        if dev_rmse*0.998 + train_loss*0.002 < min_rmse:\n",
    "            # Save model if your model improved\n",
    "            min_rmse = dev_rmse*0.998 + train_loss*0.002\n",
    "            print('epoch = {:4d}, training loss = {:.4f}, validation loss = {:.4f}, weighted average loss = {:.4f}'\n",
    "                .format(epoch + 1, train_loss, dev_rmse, min_rmse))\n",
    "            torch.save(model.state_dict(), config['save_path'])  # Save model to specified path\n",
    "            early_stop_cnt = 0\n",
    "        else:\n",
    "            early_stop_cnt += 1\n",
    "\n",
    "        epoch += 1\n",
    "        loss_record['dev'].append(dev_rmse)\n",
    "        if early_stop_cnt > config['early_stop']:\n",
    "            # Stop training if your model stops improving for \"config['early_stop']\" epochs.\n",
    "            break\n",
    "\n",
    "    print('Finished training after {} epochs'.format(epoch))\n",
    "    return min_rmse, loss_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hSd4Bn3O2PL"
   },
   "source": [
    "## **Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yrxrD3YsN3U2"
   },
   "outputs": [],
   "source": [
    "def dev(dv_set, model, device):\n",
    "    model.eval()                                # set model to evalutation mode\n",
    "    total_loss = 0\n",
    "    for x, y in dv_set:                         # iterate through the dataloader\n",
    "        x, y = x.to(device), y.to(device)       # move data to device (cpu/cuda)\n",
    "        with torch.no_grad():                   # disable gradient calculation\n",
    "            pred = model(x)                     # forward pass (compute output)\n",
    "            rmse_l1_loss, rmse_loss = model.cal_loss(pred, y)  # compute loss\n",
    "        total_loss += rmse_loss.detach().cpu().item() * len(x)  # accumulate loss\n",
    "    total_loss = total_loss / len(dv_set.dataset)              # compute averaged loss\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0pdrhQAO41L"
   },
   "source": [
    "## **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "aSBMRFlYN5tB"
   },
   "outputs": [],
   "source": [
    "def test(tt_set, model, device):\n",
    "    model.eval()                                # set model to evalutation mode\n",
    "    preds = []\n",
    "    for x in tt_set:                            # iterate through the dataloader\n",
    "        x = x.to(device)                        # move data to device (cpu/cuda)\n",
    "        with torch.no_grad():                   # disable gradient calculation\n",
    "            pred = model(x)                     # forward pass (compute output)\n",
    "            preds.append(pred.detach().cpu())   # collect prediction\n",
    "    preds = torch.cat(preds, dim=0).numpy()     # concatenate all predictions and convert to a numpy array\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvckkF5dvf0j"
   },
   "source": [
    "# **Setup Hyper-parameters**\n",
    "\n",
    "`config` contains hyper-parameters for training and the path to save your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NPXpdumwPjE7"
   },
   "outputs": [],
   "source": [
    "device = get_device()                 # get the current available device ('cpu' or 'cuda')\n",
    "os.makedirs('models', exist_ok=True)  # The trained model will be saved to ./models/\n",
    "target_only = True                   # TODO: Using 40 states & 2 tested_positive features\n",
    "\n",
    "# TODO: How to tune these hyper-parameters to improve your model's performance?\n",
    "config = {\n",
    "    'n_epochs': 8000,                # maximum number of epochs\n",
    "    'batch_size': 130,               # mini-batch size for dataloader\n",
    "    'optimizer': 'SGD',              # optimization algorithm (optimizer in torch.optim)\n",
    "    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
    "        'lr': 5e-4,                 # learning rate of SGD\n",
    "        'momentum': 0.9,              # momentum for SGD\n",
    "        'weight_decay': 5e-4         #weight decay for SGD\n",
    "    },\n",
    "    'early_stop': 800,               # early stopping epochs (the number epochs since your model's last improvement)\n",
    "    'save_path': 'models/model.pth'  # your model will be saved here\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6j1eOV3TOH-j"
   },
   "source": [
    "# **Load data and model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eNrYBMmePLKm",
    "outputId": "fcd4f175-4f7e-4306-f33c-5f8285f11dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading the train set of COVID19 Dataset (2430 samples found, each dim = 63)\n",
      "Finished reading the dev set of COVID19 Dataset (270 samples found, each dim = 63)\n",
      "Finished reading the test set of COVID19 Dataset (893 samples found, each dim = 63)\n"
     ]
    }
   ],
   "source": [
    "tr_set = prep_dataloader(tr_path, 'train', config['batch_size'], target_only=target_only)\n",
    "dv_set = prep_dataloader(tr_path, 'dev', config['batch_size'], target_only=target_only)\n",
    "tt_set = prep_dataloader(tt_path, 'test', config['batch_size'], target_only=target_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "FHylSirLP9oh"
   },
   "outputs": [],
   "source": [
    "model = NeuralNet(tr_set.dataset.dim).to(device)  # Construct model and move to device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sX2B_zgSOPTJ"
   },
   "source": [
    "# **Start Training!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GrEbUxazQAAZ",
    "outputId": "f4f3bd74-2d97-4275-b69f-6609976b91f9",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =    1, training loss = 18.0146, validation loss = 17.7022, weighted average loss = 17.7028\n",
      "epoch =    2, training loss = 17.7474, validation loss = 17.5910, weighted average loss = 17.5913\n",
      "epoch =    3, training loss = 17.8267, validation loss = 17.4585, weighted average loss = 17.4592\n",
      "epoch =    4, training loss = 18.6979, validation loss = 17.3065, weighted average loss = 17.3093\n",
      "epoch =    5, training loss = 17.1373, validation loss = 17.1280, weighted average loss = 17.1280\n",
      "epoch =    6, training loss = 16.8872, validation loss = 16.9101, weighted average loss = 16.9100\n",
      "epoch =    7, training loss = 17.7745, validation loss = 16.6326, weighted average loss = 16.6349\n",
      "epoch =    8, training loss = 17.1241, validation loss = 16.2707, weighted average loss = 16.2724\n",
      "epoch =    9, training loss = 14.6124, validation loss = 15.7838, weighted average loss = 15.7815\n",
      "epoch =   10, training loss = 13.9781, validation loss = 15.1200, weighted average loss = 15.1177\n",
      "epoch =   11, training loss = 14.6016, validation loss = 14.2093, weighted average loss = 14.2101\n",
      "epoch =   12, training loss = 12.9653, validation loss = 12.9486, weighted average loss = 12.9486\n",
      "epoch =   13, training loss = 12.8984, validation loss = 11.2283, weighted average loss = 11.2317\n",
      "epoch =   14, training loss = 9.3371, validation loss = 8.9667, weighted average loss = 8.9675\n",
      "epoch =   15, training loss = 6.3735, validation loss = 6.3587, weighted average loss = 6.3587\n",
      "epoch =   16, training loss = 4.8119, validation loss = 4.6231, weighted average loss = 4.6235\n",
      "epoch =   17, training loss = 3.5018, validation loss = 4.3516, weighted average loss = 4.3499\n",
      "epoch =   18, training loss = 4.0050, validation loss = 4.2021, weighted average loss = 4.2017\n",
      "epoch =   19, training loss = 3.9755, validation loss = 4.0656, weighted average loss = 4.0654\n",
      "epoch =   20, training loss = 3.8190, validation loss = 3.9339, weighted average loss = 3.9337\n",
      "epoch =   21, training loss = 4.1415, validation loss = 3.8101, weighted average loss = 3.8108\n",
      "epoch =   22, training loss = 3.9026, validation loss = 3.6895, weighted average loss = 3.6899\n",
      "epoch =   23, training loss = 3.7695, validation loss = 3.5748, weighted average loss = 3.5751\n",
      "epoch =   24, training loss = 3.7023, validation loss = 3.4694, weighted average loss = 3.4698\n",
      "epoch =   25, training loss = 3.5948, validation loss = 3.3675, weighted average loss = 3.3680\n",
      "epoch =   26, training loss = 3.1438, validation loss = 3.2736, weighted average loss = 3.2733\n",
      "epoch =   27, training loss = 2.8782, validation loss = 3.1866, weighted average loss = 3.1859\n",
      "epoch =   28, training loss = 2.7611, validation loss = 3.1063, weighted average loss = 3.1056\n",
      "epoch =   29, training loss = 2.5786, validation loss = 3.0353, weighted average loss = 3.0344\n",
      "epoch =   30, training loss = 3.2124, validation loss = 2.9688, weighted average loss = 2.9693\n",
      "epoch =   31, training loss = 2.9937, validation loss = 2.9077, weighted average loss = 2.9079\n",
      "epoch =   32, training loss = 2.7041, validation loss = 2.8558, weighted average loss = 2.8555\n",
      "epoch =   33, training loss = 2.7416, validation loss = 2.8015, weighted average loss = 2.8014\n",
      "epoch =   34, training loss = 2.7355, validation loss = 2.7563, weighted average loss = 2.7563\n",
      "epoch =   35, training loss = 2.6368, validation loss = 2.7138, weighted average loss = 2.7137\n",
      "epoch =   36, training loss = 2.6957, validation loss = 2.6730, weighted average loss = 2.6730\n",
      "epoch =   37, training loss = 2.6306, validation loss = 2.6362, weighted average loss = 2.6362\n",
      "epoch =   38, training loss = 2.5637, validation loss = 2.6009, weighted average loss = 2.6009\n",
      "epoch =   39, training loss = 2.3369, validation loss = 2.5635, weighted average loss = 2.5630\n",
      "epoch =   40, training loss = 2.6126, validation loss = 2.5314, weighted average loss = 2.5316\n",
      "epoch =   41, training loss = 2.9557, validation loss = 2.5015, weighted average loss = 2.5024\n",
      "epoch =   42, training loss = 2.4479, validation loss = 2.4664, weighted average loss = 2.4664\n",
      "epoch =   43, training loss = 2.5492, validation loss = 2.4365, weighted average loss = 2.4367\n",
      "epoch =   44, training loss = 2.5255, validation loss = 2.4042, weighted average loss = 2.4045\n",
      "epoch =   45, training loss = 2.1899, validation loss = 2.3768, weighted average loss = 2.3764\n",
      "epoch =   46, training loss = 2.1338, validation loss = 2.3448, weighted average loss = 2.3443\n",
      "epoch =   47, training loss = 2.4591, validation loss = 2.3164, weighted average loss = 2.3167\n",
      "epoch =   48, training loss = 2.7096, validation loss = 2.2888, weighted average loss = 2.2896\n",
      "epoch =   49, training loss = 2.4844, validation loss = 2.2577, weighted average loss = 2.2582\n",
      "epoch =   50, training loss = 2.3112, validation loss = 2.2311, weighted average loss = 2.2312\n",
      "epoch =   51, training loss = 1.9834, validation loss = 2.2015, weighted average loss = 2.2011\n",
      "epoch =   52, training loss = 2.0783, validation loss = 2.1752, weighted average loss = 2.1750\n",
      "epoch =   53, training loss = 1.9587, validation loss = 2.1473, weighted average loss = 2.1469\n",
      "epoch =   54, training loss = 2.0324, validation loss = 2.1187, weighted average loss = 2.1185\n",
      "epoch =   55, training loss = 2.0657, validation loss = 2.0932, weighted average loss = 2.0931\n",
      "epoch =   56, training loss = 1.7552, validation loss = 2.0649, weighted average loss = 2.0643\n",
      "epoch =   57, training loss = 1.8782, validation loss = 2.0406, weighted average loss = 2.0402\n",
      "epoch =   58, training loss = 2.1231, validation loss = 2.0151, weighted average loss = 2.0154\n",
      "epoch =   59, training loss = 2.1559, validation loss = 1.9881, weighted average loss = 1.9884\n",
      "epoch =   60, training loss = 2.0265, validation loss = 1.9633, weighted average loss = 1.9634\n",
      "epoch =   61, training loss = 1.8584, validation loss = 1.9380, weighted average loss = 1.9378\n",
      "epoch =   62, training loss = 1.9295, validation loss = 1.9136, weighted average loss = 1.9136\n",
      "epoch =   63, training loss = 1.9224, validation loss = 1.8892, weighted average loss = 1.8892\n",
      "epoch =   64, training loss = 1.8806, validation loss = 1.8660, weighted average loss = 1.8660\n",
      "epoch =   65, training loss = 2.0241, validation loss = 1.8435, weighted average loss = 1.8439\n",
      "epoch =   66, training loss = 1.6768, validation loss = 1.8201, weighted average loss = 1.8198\n",
      "epoch =   67, training loss = 1.7906, validation loss = 1.7991, weighted average loss = 1.7991\n",
      "epoch =   68, training loss = 1.7158, validation loss = 1.7761, weighted average loss = 1.7760\n",
      "epoch =   69, training loss = 1.7496, validation loss = 1.7585, weighted average loss = 1.7585\n",
      "epoch =   70, training loss = 1.8429, validation loss = 1.7346, weighted average loss = 1.7348\n",
      "epoch =   71, training loss = 1.7285, validation loss = 1.7140, weighted average loss = 1.7141\n",
      "epoch =   72, training loss = 1.7553, validation loss = 1.6954, weighted average loss = 1.6955\n",
      "epoch =   73, training loss = 1.6358, validation loss = 1.6746, weighted average loss = 1.6745\n",
      "epoch =   74, training loss = 1.5270, validation loss = 1.6591, weighted average loss = 1.6588\n",
      "epoch =   75, training loss = 1.9662, validation loss = 1.6436, weighted average loss = 1.6443\n",
      "epoch =   76, training loss = 1.4265, validation loss = 1.6217, weighted average loss = 1.6213\n",
      "epoch =   77, training loss = 1.3702, validation loss = 1.6073, weighted average loss = 1.6068\n",
      "epoch =   78, training loss = 1.7169, validation loss = 1.5909, weighted average loss = 1.5912\n",
      "epoch =   79, training loss = 1.5390, validation loss = 1.5756, weighted average loss = 1.5756\n",
      "epoch =   80, training loss = 1.5297, validation loss = 1.5621, weighted average loss = 1.5620\n",
      "epoch =   81, training loss = 1.4341, validation loss = 1.5469, weighted average loss = 1.5467\n",
      "epoch =   82, training loss = 1.3448, validation loss = 1.5401, weighted average loss = 1.5397\n",
      "epoch =   83, training loss = 1.7065, validation loss = 1.5210, weighted average loss = 1.5214\n",
      "epoch =   84, training loss = 1.5449, validation loss = 1.5117, weighted average loss = 1.5118\n",
      "epoch =   85, training loss = 1.4548, validation loss = 1.4975, weighted average loss = 1.4974\n",
      "epoch =   86, training loss = 1.3747, validation loss = 1.4883, weighted average loss = 1.4881\n",
      "epoch =   87, training loss = 1.4396, validation loss = 1.4772, weighted average loss = 1.4771\n",
      "epoch =   88, training loss = 1.4536, validation loss = 1.4691, weighted average loss = 1.4690\n",
      "epoch =   89, training loss = 1.4806, validation loss = 1.4575, weighted average loss = 1.4576\n",
      "epoch =   90, training loss = 1.4055, validation loss = 1.4530, weighted average loss = 1.4529\n",
      "epoch =   91, training loss = 1.7143, validation loss = 1.4446, weighted average loss = 1.4452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =   92, training loss = 1.4158, validation loss = 1.4356, weighted average loss = 1.4356\n",
      "epoch =   93, training loss = 1.4008, validation loss = 1.4279, weighted average loss = 1.4278\n",
      "epoch =   94, training loss = 1.3669, validation loss = 1.4218, weighted average loss = 1.4217\n",
      "epoch =   95, training loss = 1.2738, validation loss = 1.4128, weighted average loss = 1.4126\n",
      "epoch =   96, training loss = 1.4385, validation loss = 1.4079, weighted average loss = 1.4079\n",
      "epoch =   97, training loss = 1.4575, validation loss = 1.4025, weighted average loss = 1.4026\n",
      "epoch =   98, training loss = 1.4607, validation loss = 1.4004, weighted average loss = 1.4006\n",
      "epoch =   99, training loss = 1.4509, validation loss = 1.3907, weighted average loss = 1.3908\n",
      "epoch =  100, training loss = 1.3691, validation loss = 1.3880, weighted average loss = 1.3880\n",
      "epoch =  101, training loss = 1.4349, validation loss = 1.3824, weighted average loss = 1.3825\n",
      "epoch =  102, training loss = 1.4493, validation loss = 1.3817, weighted average loss = 1.3818\n",
      "epoch =  103, training loss = 1.2397, validation loss = 1.3752, weighted average loss = 1.3750\n",
      "epoch =  104, training loss = 1.3155, validation loss = 1.3716, weighted average loss = 1.3715\n",
      "epoch =  105, training loss = 1.2894, validation loss = 1.3673, weighted average loss = 1.3671\n",
      "epoch =  106, training loss = 1.3376, validation loss = 1.3625, weighted average loss = 1.3624\n",
      "epoch =  107, training loss = 1.3061, validation loss = 1.3605, weighted average loss = 1.3604\n",
      "epoch =  108, training loss = 1.3677, validation loss = 1.3556, weighted average loss = 1.3556\n",
      "epoch =  109, training loss = 1.1685, validation loss = 1.3543, weighted average loss = 1.3539\n",
      "epoch =  110, training loss = 1.3474, validation loss = 1.3485, weighted average loss = 1.3485\n",
      "epoch =  112, training loss = 1.3806, validation loss = 1.3410, weighted average loss = 1.3410\n",
      "epoch =  115, training loss = 1.1806, validation loss = 1.3333, weighted average loss = 1.3330\n",
      "epoch =  117, training loss = 1.2806, validation loss = 1.3290, weighted average loss = 1.3289\n",
      "epoch =  118, training loss = 1.4209, validation loss = 1.3265, weighted average loss = 1.3267\n",
      "epoch =  119, training loss = 1.3080, validation loss = 1.3226, weighted average loss = 1.3225\n",
      "epoch =  121, training loss = 1.1619, validation loss = 1.3164, weighted average loss = 1.3161\n",
      "epoch =  123, training loss = 1.1808, validation loss = 1.3113, weighted average loss = 1.3110\n",
      "epoch =  124, training loss = 1.4948, validation loss = 1.3099, weighted average loss = 1.3103\n",
      "epoch =  125, training loss = 1.2333, validation loss = 1.3070, weighted average loss = 1.3069\n",
      "epoch =  126, training loss = 1.4140, validation loss = 1.3042, weighted average loss = 1.3045\n",
      "epoch =  127, training loss = 1.1788, validation loss = 1.3030, weighted average loss = 1.3028\n",
      "epoch =  128, training loss = 1.3609, validation loss = 1.3013, weighted average loss = 1.3014\n",
      "epoch =  130, training loss = 1.4090, validation loss = 1.2937, weighted average loss = 1.2939\n",
      "epoch =  131, training loss = 1.2192, validation loss = 1.2940, weighted average loss = 1.2938\n",
      "epoch =  132, training loss = 1.3951, validation loss = 1.2892, weighted average loss = 1.2894\n",
      "epoch =  134, training loss = 1.0332, validation loss = 1.2848, weighted average loss = 1.2843\n",
      "epoch =  137, training loss = 1.0491, validation loss = 1.2819, weighted average loss = 1.2815\n",
      "epoch =  138, training loss = 1.2008, validation loss = 1.2771, weighted average loss = 1.2769\n",
      "epoch =  139, training loss = 1.2781, validation loss = 1.2743, weighted average loss = 1.2743\n",
      "epoch =  140, training loss = 1.3865, validation loss = 1.2721, weighted average loss = 1.2723\n",
      "epoch =  141, training loss = 1.2640, validation loss = 1.2710, weighted average loss = 1.2710\n",
      "epoch =  142, training loss = 1.3349, validation loss = 1.2667, weighted average loss = 1.2668\n",
      "epoch =  143, training loss = 1.2996, validation loss = 1.2653, weighted average loss = 1.2654\n",
      "epoch =  144, training loss = 1.2134, validation loss = 1.2638, weighted average loss = 1.2637\n",
      "epoch =  145, training loss = 1.2938, validation loss = 1.2600, weighted average loss = 1.2601\n",
      "epoch =  146, training loss = 0.9439, validation loss = 1.2578, weighted average loss = 1.2571\n",
      "epoch =  148, training loss = 1.2929, validation loss = 1.2558, weighted average loss = 1.2558\n",
      "epoch =  149, training loss = 1.1604, validation loss = 1.2548, weighted average loss = 1.2546\n",
      "epoch =  150, training loss = 1.2322, validation loss = 1.2544, weighted average loss = 1.2544\n",
      "epoch =  151, training loss = 1.4489, validation loss = 1.2487, weighted average loss = 1.2491\n",
      "epoch =  152, training loss = 1.1681, validation loss = 1.2476, weighted average loss = 1.2475\n",
      "epoch =  153, training loss = 1.3667, validation loss = 1.2442, weighted average loss = 1.2444\n",
      "epoch =  155, training loss = 0.9317, validation loss = 1.2405, weighted average loss = 1.2399\n",
      "epoch =  156, training loss = 1.1177, validation loss = 1.2365, weighted average loss = 1.2363\n",
      "epoch =  158, training loss = 1.2292, validation loss = 1.2326, weighted average loss = 1.2326\n",
      "epoch =  160, training loss = 1.1125, validation loss = 1.2301, weighted average loss = 1.2299\n",
      "epoch =  161, training loss = 1.3334, validation loss = 1.2280, weighted average loss = 1.2282\n",
      "epoch =  164, training loss = 1.2656, validation loss = 1.2234, weighted average loss = 1.2235\n",
      "epoch =  165, training loss = 1.1602, validation loss = 1.2211, weighted average loss = 1.2209\n",
      "epoch =  167, training loss = 1.0490, validation loss = 1.2193, weighted average loss = 1.2190\n",
      "epoch =  168, training loss = 1.2718, validation loss = 1.2138, weighted average loss = 1.2139\n",
      "epoch =  169, training loss = 1.2856, validation loss = 1.2113, weighted average loss = 1.2114\n",
      "epoch =  171, training loss = 1.1631, validation loss = 1.2075, weighted average loss = 1.2075\n",
      "epoch =  173, training loss = 1.1936, validation loss = 1.2065, weighted average loss = 1.2065\n",
      "epoch =  174, training loss = 1.1305, validation loss = 1.2025, weighted average loss = 1.2024\n",
      "epoch =  176, training loss = 1.2008, validation loss = 1.2017, weighted average loss = 1.2017\n",
      "epoch =  177, training loss = 1.1007, validation loss = 1.1970, weighted average loss = 1.1968\n",
      "epoch =  179, training loss = 1.2905, validation loss = 1.1944, weighted average loss = 1.1946\n",
      "epoch =  180, training loss = 1.1303, validation loss = 1.1939, weighted average loss = 1.1938\n",
      "epoch =  181, training loss = 1.1619, validation loss = 1.1911, weighted average loss = 1.1910\n",
      "epoch =  183, training loss = 1.0622, validation loss = 1.1873, weighted average loss = 1.1871\n",
      "epoch =  185, training loss = 1.0063, validation loss = 1.1845, weighted average loss = 1.1842\n",
      "epoch =  187, training loss = 1.1010, validation loss = 1.1807, weighted average loss = 1.1805\n",
      "epoch =  190, training loss = 1.1022, validation loss = 1.1791, weighted average loss = 1.1790\n",
      "epoch =  192, training loss = 1.3774, validation loss = 1.1724, weighted average loss = 1.1728\n",
      "epoch =  194, training loss = 1.0839, validation loss = 1.1713, weighted average loss = 1.1711\n",
      "epoch =  198, training loss = 0.9865, validation loss = 1.1699, weighted average loss = 1.1695\n",
      "epoch =  200, training loss = 1.1933, validation loss = 1.1614, weighted average loss = 1.1614\n",
      "epoch =  202, training loss = 0.9954, validation loss = 1.1591, weighted average loss = 1.1588\n",
      "epoch =  203, training loss = 1.1425, validation loss = 1.1584, weighted average loss = 1.1584\n",
      "epoch =  204, training loss = 0.9243, validation loss = 1.1577, weighted average loss = 1.1573\n",
      "epoch =  205, training loss = 1.0476, validation loss = 1.1573, weighted average loss = 1.1570\n",
      "epoch =  206, training loss = 1.2395, validation loss = 1.1522, weighted average loss = 1.1524\n",
      "epoch =  209, training loss = 1.1346, validation loss = 1.1501, weighted average loss = 1.1501\n",
      "epoch =  210, training loss = 1.1784, validation loss = 1.1496, weighted average loss = 1.1497\n",
      "epoch =  211, training loss = 1.0103, validation loss = 1.1496, weighted average loss = 1.1493\n",
      "epoch =  212, training loss = 1.1294, validation loss = 1.1483, weighted average loss = 1.1482\n",
      "epoch =  214, training loss = 1.1428, validation loss = 1.1423, weighted average loss = 1.1423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  216, training loss = 1.2525, validation loss = 1.1405, weighted average loss = 1.1408\n",
      "epoch =  218, training loss = 1.0534, validation loss = 1.1373, weighted average loss = 1.1372\n",
      "epoch =  221, training loss = 1.1156, validation loss = 1.1335, weighted average loss = 1.1335\n",
      "epoch =  223, training loss = 1.0572, validation loss = 1.1311, weighted average loss = 1.1309\n",
      "epoch =  225, training loss = 1.0764, validation loss = 1.1283, weighted average loss = 1.1281\n",
      "epoch =  227, training loss = 1.1629, validation loss = 1.1278, weighted average loss = 1.1279\n",
      "epoch =  231, training loss = 1.1767, validation loss = 1.1215, weighted average loss = 1.1216\n",
      "epoch =  232, training loss = 1.1996, validation loss = 1.1214, weighted average loss = 1.1216\n",
      "epoch =  233, training loss = 0.9878, validation loss = 1.1204, weighted average loss = 1.1201\n",
      "epoch =  235, training loss = 0.9801, validation loss = 1.1182, weighted average loss = 1.1179\n",
      "epoch =  236, training loss = 1.1549, validation loss = 1.1177, weighted average loss = 1.1177\n",
      "epoch =  237, training loss = 1.0996, validation loss = 1.1159, weighted average loss = 1.1158\n",
      "epoch =  241, training loss = 1.1475, validation loss = 1.1128, weighted average loss = 1.1129\n",
      "epoch =  244, training loss = 1.2335, validation loss = 1.1105, weighted average loss = 1.1108\n",
      "epoch =  246, training loss = 1.0126, validation loss = 1.1082, weighted average loss = 1.1080\n",
      "epoch =  248, training loss = 1.2660, validation loss = 1.1043, weighted average loss = 1.1047\n",
      "epoch =  250, training loss = 1.0940, validation loss = 1.1027, weighted average loss = 1.1027\n",
      "epoch =  253, training loss = 1.1828, validation loss = 1.1012, weighted average loss = 1.1014\n",
      "epoch =  255, training loss = 0.9543, validation loss = 1.0997, weighted average loss = 1.0994\n",
      "epoch =  258, training loss = 1.2339, validation loss = 1.0966, weighted average loss = 1.0968\n",
      "epoch =  260, training loss = 0.8750, validation loss = 1.0932, weighted average loss = 1.0928\n",
      "epoch =  262, training loss = 0.9646, validation loss = 1.0928, weighted average loss = 1.0925\n",
      "epoch =  264, training loss = 1.2977, validation loss = 1.0901, weighted average loss = 1.0905\n",
      "epoch =  265, training loss = 1.1082, validation loss = 1.0892, weighted average loss = 1.0892\n",
      "epoch =  268, training loss = 1.1329, validation loss = 1.0871, weighted average loss = 1.0872\n",
      "epoch =  272, training loss = 1.0235, validation loss = 1.0853, weighted average loss = 1.0851\n",
      "epoch =  273, training loss = 1.0572, validation loss = 1.0837, weighted average loss = 1.0837\n",
      "epoch =  278, training loss = 1.1488, validation loss = 1.0833, weighted average loss = 1.0835\n",
      "epoch =  279, training loss = 0.9074, validation loss = 1.0782, weighted average loss = 1.0779\n",
      "epoch =  282, training loss = 1.1142, validation loss = 1.0764, weighted average loss = 1.0765\n",
      "epoch =  285, training loss = 0.9044, validation loss = 1.0759, weighted average loss = 1.0755\n",
      "epoch =  288, training loss = 1.1111, validation loss = 1.0730, weighted average loss = 1.0731\n",
      "epoch =  291, training loss = 0.9837, validation loss = 1.0702, weighted average loss = 1.0700\n",
      "epoch =  297, training loss = 0.9968, validation loss = 1.0667, weighted average loss = 1.0665\n",
      "epoch =  303, training loss = 1.2354, validation loss = 1.0635, weighted average loss = 1.0638\n",
      "epoch =  305, training loss = 1.0705, validation loss = 1.0628, weighted average loss = 1.0628\n",
      "epoch =  311, training loss = 0.9641, validation loss = 1.0621, weighted average loss = 1.0619\n",
      "epoch =  312, training loss = 1.0598, validation loss = 1.0603, weighted average loss = 1.0603\n",
      "epoch =  313, training loss = 0.7950, validation loss = 1.0575, weighted average loss = 1.0570\n",
      "epoch =  316, training loss = 1.0575, validation loss = 1.0558, weighted average loss = 1.0558\n",
      "epoch =  322, training loss = 1.0839, validation loss = 1.0542, weighted average loss = 1.0542\n",
      "epoch =  323, training loss = 0.9165, validation loss = 1.0523, weighted average loss = 1.0520\n",
      "epoch =  329, training loss = 1.1695, validation loss = 1.0516, weighted average loss = 1.0518\n",
      "epoch =  332, training loss = 1.1731, validation loss = 1.0482, weighted average loss = 1.0484\n",
      "epoch =  333, training loss = 0.8886, validation loss = 1.0475, weighted average loss = 1.0472\n",
      "epoch =  335, training loss = 0.9851, validation loss = 1.0460, weighted average loss = 1.0459\n",
      "epoch =  339, training loss = 1.0494, validation loss = 1.0453, weighted average loss = 1.0453\n",
      "epoch =  341, training loss = 1.0217, validation loss = 1.0436, weighted average loss = 1.0435\n",
      "epoch =  349, training loss = 1.0680, validation loss = 1.0402, weighted average loss = 1.0402\n",
      "epoch =  355, training loss = 1.1489, validation loss = 1.0384, weighted average loss = 1.0386\n",
      "epoch =  356, training loss = 1.3507, validation loss = 1.0376, weighted average loss = 1.0383\n",
      "epoch =  360, training loss = 1.1153, validation loss = 1.0375, weighted average loss = 1.0377\n",
      "epoch =  361, training loss = 0.9261, validation loss = 1.0375, weighted average loss = 1.0373\n",
      "epoch =  364, training loss = 1.0729, validation loss = 1.0349, weighted average loss = 1.0349\n",
      "epoch =  367, training loss = 0.8999, validation loss = 1.0343, weighted average loss = 1.0341\n",
      "epoch =  371, training loss = 1.1458, validation loss = 1.0336, weighted average loss = 1.0338\n",
      "epoch =  372, training loss = 1.1063, validation loss = 1.0318, weighted average loss = 1.0319\n",
      "epoch =  374, training loss = 0.9766, validation loss = 1.0314, weighted average loss = 1.0313\n",
      "epoch =  380, training loss = 1.1352, validation loss = 1.0299, weighted average loss = 1.0301\n",
      "epoch =  381, training loss = 0.9856, validation loss = 1.0295, weighted average loss = 1.0294\n",
      "epoch =  382, training loss = 0.9994, validation loss = 1.0290, weighted average loss = 1.0290\n",
      "epoch =  388, training loss = 1.1260, validation loss = 1.0273, weighted average loss = 1.0275\n",
      "epoch =  389, training loss = 0.9730, validation loss = 1.0274, weighted average loss = 1.0273\n",
      "epoch =  391, training loss = 1.0980, validation loss = 1.0252, weighted average loss = 1.0253\n",
      "epoch =  398, training loss = 0.9641, validation loss = 1.0243, weighted average loss = 1.0241\n",
      "epoch =  399, training loss = 0.8660, validation loss = 1.0223, weighted average loss = 1.0220\n",
      "epoch =  405, training loss = 1.1021, validation loss = 1.0206, weighted average loss = 1.0207\n",
      "epoch =  408, training loss = 1.1035, validation loss = 1.0200, weighted average loss = 1.0202\n",
      "epoch =  409, training loss = 0.9507, validation loss = 1.0201, weighted average loss = 1.0200\n",
      "epoch =  412, training loss = 1.0384, validation loss = 1.0191, weighted average loss = 1.0191\n",
      "epoch =  425, training loss = 0.9781, validation loss = 1.0153, weighted average loss = 1.0152\n",
      "epoch =  436, training loss = 0.8622, validation loss = 1.0151, weighted average loss = 1.0148\n",
      "epoch =  440, training loss = 1.0729, validation loss = 1.0110, weighted average loss = 1.0112\n",
      "epoch =  448, training loss = 1.0848, validation loss = 1.0094, weighted average loss = 1.0096\n",
      "epoch =  451, training loss = 1.0117, validation loss = 1.0081, weighted average loss = 1.0081\n",
      "epoch =  461, training loss = 0.8693, validation loss = 1.0060, weighted average loss = 1.0057\n",
      "epoch =  474, training loss = 1.0580, validation loss = 1.0034, weighted average loss = 1.0035\n",
      "epoch =  484, training loss = 0.9262, validation loss = 1.0009, weighted average loss = 1.0007\n",
      "epoch =  491, training loss = 1.0116, validation loss = 0.9993, weighted average loss = 0.9994\n",
      "epoch =  499, training loss = 1.1960, validation loss = 0.9988, weighted average loss = 0.9992\n",
      "epoch =  503, training loss = 0.9976, validation loss = 0.9991, weighted average loss = 0.9991\n",
      "epoch =  505, training loss = 0.9228, validation loss = 0.9980, weighted average loss = 0.9978\n",
      "epoch =  509, training loss = 0.9192, validation loss = 0.9973, weighted average loss = 0.9971\n",
      "epoch =  515, training loss = 0.9655, validation loss = 0.9950, weighted average loss = 0.9949\n",
      "epoch =  522, training loss = 0.9278, validation loss = 0.9938, weighted average loss = 0.9936\n",
      "epoch =  524, training loss = 1.1615, validation loss = 0.9929, weighted average loss = 0.9933\n",
      "epoch =  527, training loss = 0.9127, validation loss = 0.9932, weighted average loss = 0.9930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  530, training loss = 0.8908, validation loss = 0.9921, weighted average loss = 0.9919\n",
      "epoch =  534, training loss = 0.8555, validation loss = 0.9915, weighted average loss = 0.9913\n",
      "epoch =  550, training loss = 0.8710, validation loss = 0.9898, weighted average loss = 0.9896\n",
      "epoch =  557, training loss = 0.9584, validation loss = 0.9891, weighted average loss = 0.9891\n",
      "epoch =  566, training loss = 1.0242, validation loss = 0.9858, weighted average loss = 0.9859\n",
      "epoch =  569, training loss = 1.0134, validation loss = 0.9849, weighted average loss = 0.9849\n",
      "epoch =  576, training loss = 0.8974, validation loss = 0.9848, weighted average loss = 0.9846\n",
      "epoch =  582, training loss = 1.0517, validation loss = 0.9827, weighted average loss = 0.9828\n",
      "epoch =  588, training loss = 0.8515, validation loss = 0.9828, weighted average loss = 0.9825\n",
      "epoch =  592, training loss = 0.8253, validation loss = 0.9821, weighted average loss = 0.9817\n",
      "epoch =  603, training loss = 0.9941, validation loss = 0.9803, weighted average loss = 0.9803\n",
      "epoch =  612, training loss = 1.2223, validation loss = 0.9794, weighted average loss = 0.9799\n",
      "epoch =  620, training loss = 0.9062, validation loss = 0.9775, weighted average loss = 0.9774\n",
      "epoch =  623, training loss = 0.9412, validation loss = 0.9772, weighted average loss = 0.9771\n",
      "epoch =  629, training loss = 1.0125, validation loss = 0.9760, weighted average loss = 0.9761\n",
      "epoch =  633, training loss = 0.9391, validation loss = 0.9758, weighted average loss = 0.9757\n",
      "epoch =  646, training loss = 0.8783, validation loss = 0.9758, weighted average loss = 0.9756\n",
      "epoch =  655, training loss = 1.1089, validation loss = 0.9742, weighted average loss = 0.9745\n",
      "epoch =  659, training loss = 0.9447, validation loss = 0.9725, weighted average loss = 0.9724\n",
      "epoch =  661, training loss = 0.9924, validation loss = 0.9721, weighted average loss = 0.9721\n",
      "epoch =  668, training loss = 1.0470, validation loss = 0.9712, weighted average loss = 0.9714\n",
      "epoch =  671, training loss = 0.9808, validation loss = 0.9712, weighted average loss = 0.9712\n",
      "epoch =  681, training loss = 0.8834, validation loss = 0.9712, weighted average loss = 0.9711\n",
      "epoch =  682, training loss = 0.9348, validation loss = 0.9700, weighted average loss = 0.9700\n",
      "epoch =  683, training loss = 0.7858, validation loss = 0.9693, weighted average loss = 0.9689\n",
      "epoch =  687, training loss = 0.9173, validation loss = 0.9687, weighted average loss = 0.9686\n",
      "epoch =  690, training loss = 1.1816, validation loss = 0.9679, weighted average loss = 0.9683\n",
      "epoch =  695, training loss = 0.9431, validation loss = 0.9677, weighted average loss = 0.9677\n",
      "epoch =  696, training loss = 0.8667, validation loss = 0.9678, weighted average loss = 0.9676\n",
      "epoch =  704, training loss = 0.8016, validation loss = 0.9669, weighted average loss = 0.9665\n",
      "epoch =  717, training loss = 1.0444, validation loss = 0.9649, weighted average loss = 0.9650\n",
      "epoch =  721, training loss = 0.9087, validation loss = 0.9643, weighted average loss = 0.9642\n",
      "epoch =  732, training loss = 1.0396, validation loss = 0.9638, weighted average loss = 0.9640\n",
      "epoch =  738, training loss = 0.9693, validation loss = 0.9633, weighted average loss = 0.9633\n",
      "epoch =  739, training loss = 0.8236, validation loss = 0.9630, weighted average loss = 0.9627\n",
      "epoch =  741, training loss = 1.0199, validation loss = 0.9624, weighted average loss = 0.9625\n",
      "epoch =  751, training loss = 0.8900, validation loss = 0.9611, weighted average loss = 0.9610\n",
      "epoch =  758, training loss = 0.9026, validation loss = 0.9606, weighted average loss = 0.9605\n",
      "epoch =  768, training loss = 0.7519, validation loss = 0.9597, weighted average loss = 0.9593\n",
      "epoch =  786, training loss = 0.9591, validation loss = 0.9585, weighted average loss = 0.9585\n",
      "epoch =  787, training loss = 0.8360, validation loss = 0.9571, weighted average loss = 0.9569\n",
      "epoch =  791, training loss = 0.9200, validation loss = 0.9564, weighted average loss = 0.9564\n",
      "epoch =  804, training loss = 1.0410, validation loss = 0.9557, weighted average loss = 0.9559\n",
      "epoch =  805, training loss = 1.0813, validation loss = 0.9552, weighted average loss = 0.9554\n",
      "epoch =  814, training loss = 1.1510, validation loss = 0.9548, weighted average loss = 0.9552\n",
      "epoch =  817, training loss = 1.0652, validation loss = 0.9536, weighted average loss = 0.9539\n",
      "epoch =  821, training loss = 0.8512, validation loss = 0.9536, weighted average loss = 0.9534\n",
      "epoch =  836, training loss = 0.8773, validation loss = 0.9529, weighted average loss = 0.9528\n",
      "epoch =  838, training loss = 0.9163, validation loss = 0.9525, weighted average loss = 0.9524\n",
      "epoch =  849, training loss = 0.7727, validation loss = 0.9516, weighted average loss = 0.9512\n",
      "epoch =  866, training loss = 0.9849, validation loss = 0.9511, weighted average loss = 0.9512\n",
      "epoch =  869, training loss = 0.9882, validation loss = 0.9500, weighted average loss = 0.9501\n",
      "epoch =  883, training loss = 1.1203, validation loss = 0.9486, weighted average loss = 0.9490\n",
      "epoch =  894, training loss = 0.9348, validation loss = 0.9489, weighted average loss = 0.9489\n",
      "epoch =  897, training loss = 0.9700, validation loss = 0.9476, weighted average loss = 0.9477\n",
      "epoch =  900, training loss = 0.9563, validation loss = 0.9474, weighted average loss = 0.9474\n",
      "epoch =  905, training loss = 0.8903, validation loss = 0.9467, weighted average loss = 0.9466\n",
      "epoch =  914, training loss = 0.8773, validation loss = 0.9456, weighted average loss = 0.9455\n",
      "epoch =  919, training loss = 0.9849, validation loss = 0.9452, weighted average loss = 0.9453\n",
      "epoch =  920, training loss = 1.1012, validation loss = 0.9445, weighted average loss = 0.9448\n",
      "epoch =  931, training loss = 0.8028, validation loss = 0.9438, weighted average loss = 0.9435\n",
      "epoch =  937, training loss = 0.9688, validation loss = 0.9434, weighted average loss = 0.9435\n",
      "epoch =  945, training loss = 0.8682, validation loss = 0.9429, weighted average loss = 0.9428\n",
      "epoch =  947, training loss = 0.8583, validation loss = 0.9428, weighted average loss = 0.9426\n",
      "epoch =  953, training loss = 0.8398, validation loss = 0.9423, weighted average loss = 0.9421\n",
      "epoch =  966, training loss = 0.8165, validation loss = 0.9411, weighted average loss = 0.9408\n",
      "epoch =  979, training loss = 1.0015, validation loss = 0.9402, weighted average loss = 0.9403\n",
      "epoch =  987, training loss = 0.9085, validation loss = 0.9400, weighted average loss = 0.9399\n",
      "epoch =  991, training loss = 1.0441, validation loss = 0.9395, weighted average loss = 0.9397\n",
      "epoch =  993, training loss = 0.8971, validation loss = 0.9389, weighted average loss = 0.9388\n",
      "epoch = 1013, training loss = 0.9581, validation loss = 0.9387, weighted average loss = 0.9387\n",
      "epoch = 1016, training loss = 0.9695, validation loss = 0.9373, weighted average loss = 0.9374\n",
      "epoch = 1023, training loss = 0.8921, validation loss = 0.9371, weighted average loss = 0.9371\n",
      "epoch = 1024, training loss = 0.9234, validation loss = 0.9368, weighted average loss = 0.9368\n",
      "epoch = 1028, training loss = 0.9230, validation loss = 0.9365, weighted average loss = 0.9365\n",
      "epoch = 1035, training loss = 1.0846, validation loss = 0.9356, weighted average loss = 0.9359\n",
      "epoch = 1046, training loss = 0.8562, validation loss = 0.9355, weighted average loss = 0.9353\n",
      "epoch = 1050, training loss = 0.9326, validation loss = 0.9348, weighted average loss = 0.9348\n",
      "epoch = 1061, training loss = 0.8817, validation loss = 0.9347, weighted average loss = 0.9346\n",
      "epoch = 1064, training loss = 0.9275, validation loss = 0.9341, weighted average loss = 0.9340\n",
      "epoch = 1070, training loss = 0.8847, validation loss = 0.9341, weighted average loss = 0.9340\n",
      "epoch = 1080, training loss = 0.8901, validation loss = 0.9337, weighted average loss = 0.9336\n",
      "epoch = 1085, training loss = 0.9041, validation loss = 0.9333, weighted average loss = 0.9333\n",
      "epoch = 1094, training loss = 0.8827, validation loss = 0.9332, weighted average loss = 0.9331\n",
      "epoch = 1096, training loss = 0.9393, validation loss = 0.9324, weighted average loss = 0.9324\n",
      "epoch = 1099, training loss = 0.7324, validation loss = 0.9320, weighted average loss = 0.9316\n",
      "epoch = 1102, training loss = 0.9768, validation loss = 0.9312, weighted average loss = 0.9313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1122, training loss = 0.9835, validation loss = 0.9307, weighted average loss = 0.9308\n",
      "epoch = 1142, training loss = 0.9168, validation loss = 0.9295, weighted average loss = 0.9294\n",
      "epoch = 1156, training loss = 0.8891, validation loss = 0.9294, weighted average loss = 0.9293\n",
      "epoch = 1157, training loss = 0.8642, validation loss = 0.9288, weighted average loss = 0.9287\n",
      "epoch = 1164, training loss = 0.9776, validation loss = 0.9281, weighted average loss = 0.9282\n",
      "epoch = 1178, training loss = 0.8728, validation loss = 0.9281, weighted average loss = 0.9280\n",
      "epoch = 1186, training loss = 0.8561, validation loss = 0.9278, weighted average loss = 0.9276\n",
      "epoch = 1188, training loss = 0.9529, validation loss = 0.9271, weighted average loss = 0.9272\n",
      "epoch = 1207, training loss = 1.0715, validation loss = 0.9255, weighted average loss = 0.9258\n",
      "epoch = 1212, training loss = 0.9579, validation loss = 0.9253, weighted average loss = 0.9254\n",
      "epoch = 1214, training loss = 0.7583, validation loss = 0.9253, weighted average loss = 0.9250\n",
      "epoch = 1233, training loss = 0.7631, validation loss = 0.9247, weighted average loss = 0.9243\n",
      "epoch = 1248, training loss = 0.8640, validation loss = 0.9241, weighted average loss = 0.9240\n",
      "epoch = 1249, training loss = 0.8802, validation loss = 0.9238, weighted average loss = 0.9237\n",
      "epoch = 1262, training loss = 0.9375, validation loss = 0.9232, weighted average loss = 0.9232\n",
      "epoch = 1274, training loss = 0.7470, validation loss = 0.9232, weighted average loss = 0.9228\n",
      "epoch = 1288, training loss = 0.8948, validation loss = 0.9224, weighted average loss = 0.9223\n",
      "epoch = 1295, training loss = 1.0069, validation loss = 0.9217, weighted average loss = 0.9219\n",
      "epoch = 1300, training loss = 0.8873, validation loss = 0.9217, weighted average loss = 0.9216\n",
      "epoch = 1319, training loss = 1.1471, validation loss = 0.9205, weighted average loss = 0.9209\n",
      "epoch = 1324, training loss = 0.9735, validation loss = 0.9208, weighted average loss = 0.9209\n",
      "epoch = 1325, training loss = 1.0214, validation loss = 0.9202, weighted average loss = 0.9204\n",
      "epoch = 1326, training loss = 0.8623, validation loss = 0.9205, weighted average loss = 0.9204\n",
      "epoch = 1338, training loss = 1.1081, validation loss = 0.9194, weighted average loss = 0.9198\n",
      "epoch = 1346, training loss = 0.7036, validation loss = 0.9195, weighted average loss = 0.9191\n",
      "epoch = 1374, training loss = 0.8474, validation loss = 0.9190, weighted average loss = 0.9189\n",
      "epoch = 1378, training loss = 1.0131, validation loss = 0.9182, weighted average loss = 0.9184\n",
      "epoch = 1379, training loss = 0.8735, validation loss = 0.9178, weighted average loss = 0.9177\n",
      "epoch = 1398, training loss = 0.9952, validation loss = 0.9175, weighted average loss = 0.9177\n",
      "epoch = 1402, training loss = 0.7862, validation loss = 0.9177, weighted average loss = 0.9175\n",
      "epoch = 1406, training loss = 0.7396, validation loss = 0.9171, weighted average loss = 0.9168\n",
      "epoch = 1426, training loss = 0.9300, validation loss = 0.9165, weighted average loss = 0.9165\n",
      "epoch = 1440, training loss = 1.0125, validation loss = 0.9154, weighted average loss = 0.9156\n",
      "epoch = 1467, training loss = 1.0258, validation loss = 0.9152, weighted average loss = 0.9154\n",
      "epoch = 1491, training loss = 0.9624, validation loss = 0.9144, weighted average loss = 0.9145\n",
      "epoch = 1492, training loss = 0.8604, validation loss = 0.9137, weighted average loss = 0.9136\n",
      "epoch = 1511, training loss = 0.9251, validation loss = 0.9135, weighted average loss = 0.9135\n",
      "epoch = 1518, training loss = 0.9777, validation loss = 0.9133, weighted average loss = 0.9134\n",
      "epoch = 1553, training loss = 0.8563, validation loss = 0.9126, weighted average loss = 0.9124\n",
      "epoch = 1569, training loss = 1.1014, validation loss = 0.9120, weighted average loss = 0.9123\n",
      "epoch = 1571, training loss = 0.8253, validation loss = 0.9121, weighted average loss = 0.9120\n",
      "epoch = 1585, training loss = 0.9889, validation loss = 0.9114, weighted average loss = 0.9116\n",
      "epoch = 1596, training loss = 0.9585, validation loss = 0.9112, weighted average loss = 0.9113\n",
      "epoch = 1600, training loss = 0.9087, validation loss = 0.9112, weighted average loss = 0.9112\n",
      "epoch = 1612, training loss = 0.8773, validation loss = 0.9110, weighted average loss = 0.9110\n",
      "epoch = 1615, training loss = 1.0656, validation loss = 0.9103, weighted average loss = 0.9106\n",
      "epoch = 1638, training loss = 1.1556, validation loss = 0.9099, weighted average loss = 0.9104\n",
      "epoch = 1643, training loss = 1.0472, validation loss = 0.9099, weighted average loss = 0.9102\n",
      "epoch = 1668, training loss = 0.9574, validation loss = 0.9096, weighted average loss = 0.9097\n",
      "epoch = 1680, training loss = 0.7355, validation loss = 0.9097, weighted average loss = 0.9093\n",
      "epoch = 1692, training loss = 1.0717, validation loss = 0.9088, weighted average loss = 0.9092\n",
      "epoch = 1705, training loss = 0.8996, validation loss = 0.9081, weighted average loss = 0.9081\n",
      "epoch = 1724, training loss = 0.9548, validation loss = 0.9079, weighted average loss = 0.9080\n",
      "epoch = 1737, training loss = 0.9311, validation loss = 0.9079, weighted average loss = 0.9079\n",
      "epoch = 1745, training loss = 0.7440, validation loss = 0.9081, weighted average loss = 0.9078\n",
      "epoch = 1760, training loss = 0.8957, validation loss = 0.9077, weighted average loss = 0.9077\n",
      "epoch = 1762, training loss = 0.9546, validation loss = 0.9074, weighted average loss = 0.9075\n",
      "epoch = 1776, training loss = 0.8932, validation loss = 0.9075, weighted average loss = 0.9075\n",
      "epoch = 1785, training loss = 0.9118, validation loss = 0.9071, weighted average loss = 0.9071\n",
      "epoch = 1786, training loss = 0.9368, validation loss = 0.9067, weighted average loss = 0.9068\n",
      "epoch = 1805, training loss = 0.7025, validation loss = 0.9069, weighted average loss = 0.9065\n",
      "epoch = 1853, training loss = 0.8395, validation loss = 0.9064, weighted average loss = 0.9063\n",
      "epoch = 1856, training loss = 1.0857, validation loss = 0.9056, weighted average loss = 0.9060\n",
      "epoch = 1869, training loss = 0.8721, validation loss = 0.9059, weighted average loss = 0.9059\n",
      "epoch = 1889, training loss = 0.8824, validation loss = 0.9053, weighted average loss = 0.9052\n",
      "epoch = 1921, training loss = 0.8975, validation loss = 0.9051, weighted average loss = 0.9051\n",
      "epoch = 1923, training loss = 0.9653, validation loss = 0.9049, weighted average loss = 0.9051\n",
      "epoch = 1936, training loss = 0.8581, validation loss = 0.9048, weighted average loss = 0.9047\n",
      "epoch = 1960, training loss = 0.8252, validation loss = 0.9044, weighted average loss = 0.9042\n",
      "epoch = 1963, training loss = 0.8316, validation loss = 0.9040, weighted average loss = 0.9039\n",
      "epoch = 1999, training loss = 0.8402, validation loss = 0.9040, weighted average loss = 0.9039\n",
      "epoch = 2019, training loss = 0.8260, validation loss = 0.9040, weighted average loss = 0.9038\n",
      "epoch = 2056, training loss = 0.8477, validation loss = 0.9039, weighted average loss = 0.9037\n",
      "epoch = 2083, training loss = 0.8788, validation loss = 0.9032, weighted average loss = 0.9032\n",
      "epoch = 2085, training loss = 0.9325, validation loss = 0.9027, weighted average loss = 0.9027\n",
      "epoch = 2161, training loss = 0.7916, validation loss = 0.9027, weighted average loss = 0.9025\n",
      "epoch = 2182, training loss = 1.0231, validation loss = 0.9022, weighted average loss = 0.9024\n",
      "epoch = 2204, training loss = 0.9286, validation loss = 0.9023, weighted average loss = 0.9024\n",
      "epoch = 2212, training loss = 0.8640, validation loss = 0.9020, weighted average loss = 0.9020\n",
      "epoch = 2243, training loss = 0.9175, validation loss = 0.9013, weighted average loss = 0.9014\n",
      "epoch = 2269, training loss = 0.8782, validation loss = 0.9013, weighted average loss = 0.9013\n",
      "epoch = 2306, training loss = 0.9828, validation loss = 0.9011, weighted average loss = 0.9013\n",
      "epoch = 2340, training loss = 0.8972, validation loss = 0.9011, weighted average loss = 0.9011\n",
      "epoch = 2426, training loss = 0.9341, validation loss = 0.9005, weighted average loss = 0.9005\n",
      "epoch = 2428, training loss = 0.9566, validation loss = 0.9003, weighted average loss = 0.9004\n",
      "epoch = 2432, training loss = 0.8353, validation loss = 0.9004, weighted average loss = 0.9002\n",
      "epoch = 2489, training loss = 0.8407, validation loss = 0.9002, weighted average loss = 0.9001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 2552, training loss = 0.8558, validation loss = 0.9001, weighted average loss = 0.9000\n",
      "epoch = 2594, training loss = 0.8236, validation loss = 0.9001, weighted average loss = 0.9000\n",
      "epoch = 2600, training loss = 0.8917, validation loss = 0.8998, weighted average loss = 0.8998\n",
      "epoch = 2702, training loss = 0.8237, validation loss = 0.8996, weighted average loss = 0.8995\n",
      "epoch = 2732, training loss = 0.6978, validation loss = 0.8998, weighted average loss = 0.8994\n",
      "epoch = 2810, training loss = 0.9376, validation loss = 0.8992, weighted average loss = 0.8993\n",
      "epoch = 2897, training loss = 0.9813, validation loss = 0.8990, weighted average loss = 0.8991\n",
      "epoch = 2955, training loss = 0.8986, validation loss = 0.8986, weighted average loss = 0.8986\n",
      "epoch = 2963, training loss = 0.8666, validation loss = 0.8984, weighted average loss = 0.8984\n",
      "epoch = 3378, training loss = 0.7819, validation loss = 0.8983, weighted average loss = 0.8981\n",
      "epoch = 3422, training loss = 0.7608, validation loss = 0.8983, weighted average loss = 0.8981\n",
      "epoch = 3542, training loss = 0.8039, validation loss = 0.8982, weighted average loss = 0.8980\n",
      "epoch = 3664, training loss = 0.9736, validation loss = 0.8977, weighted average loss = 0.8978\n",
      "epoch = 3777, training loss = 0.8234, validation loss = 0.8979, weighted average loss = 0.8978\n",
      "epoch = 3846, training loss = 0.9242, validation loss = 0.8977, weighted average loss = 0.8977\n",
      "epoch = 4143, training loss = 0.6798, validation loss = 0.8979, weighted average loss = 0.8975\n",
      "epoch = 4416, training loss = 0.9257, validation loss = 0.8972, weighted average loss = 0.8973\n",
      "epoch = 4694, training loss = 0.7981, validation loss = 0.8974, weighted average loss = 0.8972\n",
      "epoch = 5086, training loss = 0.9748, validation loss = 0.8968, weighted average loss = 0.8970\n",
      "epoch = 5236, training loss = 0.8061, validation loss = 0.8971, weighted average loss = 0.8969\n",
      "epoch = 5264, training loss = 0.8067, validation loss = 0.8968, weighted average loss = 0.8967\n",
      "Finished training after 6065 epochs\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_loss_record = train(tr_set, dv_set, model, config, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "hsNO9nnXQBvP",
    "outputId": "1626def6-94c7-4a87-9447-d939f827c8eb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_learning_curve(model_loss_record, title='deep model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "3iZTVn5WQFpX",
    "outputId": "a2d5e118-559d-45c6-b644-6792af54663d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "del model\n",
    "model = NeuralNet(tr_set.dataset.dim).to(device)\n",
    "ckpt = torch.load(config['save_path'], map_location='cpu')  # Load your best model\n",
    "model.load_state_dict(ckpt)\n",
    "plot_pred(dv_set, model, device)  # Show prediction on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQikz3IPiyPf"
   },
   "source": [
    "# **Testing**\n",
    "The predictions of your model on testing set will be stored at `pred.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8cTuQjQQOon",
    "outputId": "6bc5de07-4c5a-4e87-9ae3-d09f539c5f2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to pred.csv\n"
     ]
    }
   ],
   "source": [
    "def save_pred(preds, file):\n",
    "    ''' Save predictions to specified file '''\n",
    "    print('Saving results to {}'.format(file))\n",
    "    with open(file, 'w') as fp:\n",
    "        writer = csv.writer(fp)\n",
    "        writer.writerow(['id', 'tested_positive'])\n",
    "        for i, p in enumerate(preds):\n",
    "            writer.writerow([i, p])\n",
    "\n",
    "preds = test(tt_set, model, device)  # predict COVID-19 cases with your model\n",
    "save_pred(preds, 'pred.csv')         # save prediction file to pred.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfrVxqJanGpE"
   },
   "source": [
    "# **Hints**\n",
    "\n",
    "## **Simple Baseline**\n",
    "* Run sample code\n",
    "\n",
    "## **Medium Baseline**\n",
    "* Feature selection: 40 states + 2 `tested_positive` (`TODO` in dataset)\n",
    "\n",
    "## **Strong Baseline**\n",
    "* Feature selection (what other features are useful?)\n",
    "* DNN architecture (layers? dimension? activation function?)\n",
    "* Training (mini-batch? optimizer? learning rate?)\n",
    "* L2 regularization\n",
    "* There are some mistakes in the sample code, can you find them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tmCwXgpot3t"
   },
   "source": [
    "# **Reference**\n",
    "This code is completely written by Heng-Jui Chang @ NTUEE.  \n",
    "Copying or reusing this code is required to specify the original author. \n",
    "\n",
    "E.g.  \n",
    "Source: Heng-Jui Chang @ NTUEE (https://github.com/ga642381/ML2021-Spring/blob/main/HW01/HW01.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPuibV8rME8Y2Er3TVnCm93",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "ML2021Spring - HW1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "torch(nightly)",
   "language": "python",
   "name": "conda-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
